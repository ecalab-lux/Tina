{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tina meet-up number 1: About Text Mining, 12/02/2021\n",
    "A copy of this Notebook can be downloaded from our public Github <a href=\"https://github.com/ecalab-lux/Tina\">https://github.com/ecalab-lux/Tina</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The point of view proposed here is the engineer one: first try, then evaluate, maybe later study the theoretical aspects... Does it work?\n",
    "\n",
    "![Joke](https://imgs.xkcd.com/comics/the_general_problem.png)\n",
    "(c) Xkcd https://xkcd.com/974/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use case number 1: automatic summaries with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open an ECA report from the website \n",
    "import requests\n",
    "\n",
    "response = requests.get('https://www.eca.europa.eu/lists/ecadocuments/sr21_02/sr_education_in_emergencies_en.pdf')\n",
    "print(response.status_code, response.reason, response.headers.get('Content-Type'))\n",
    "report = response.content\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert downloaded PDF to text. Note: PDF is the worst possible format for extracting textual information\n",
    "# because it was born as a format for printers and human readers\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "report_text = StringIO()\n",
    "parser = PDFParser(BytesIO(report))\n",
    "doc = PDFDocument(parser)\n",
    "rsrcmgr = PDFResourceManager()\n",
    "device = TextConverter(rsrcmgr, report_text, laparams=LAParams())\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "for page in PDFPage.create_pages(doc):\n",
    "    interpreter.process_page(page)\n",
    "report_text = report_text.getvalue()\n",
    "\n",
    "# print a portion of the text\n",
    "print(report_text[3000:3500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean-up the text\n",
    "import re\n",
    "\n",
    "report_text = re.sub(r'([^.])\\n',r'\\1 ',report_text) # Remove newlines except if they are preceded by a full stop char\n",
    "clean_text = \"\"\n",
    "for t in report_text.split('\\n'): # For each line of the text\n",
    "    if len(t)>=30: # Keep it only if the line contains at least 30 characters\n",
    "        clean_text = clean_text + t + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the summary\n",
    "from gensim.summarization import summarize # conda install gensim\n",
    "\n",
    "summary = summarize(clean_text,word_count=300) # Num word in the summary = 300\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, it is now up to the professional judgment to decide if the automatic summary is useful or not\n",
    "# Can it be used as an evidence? Probably not\n",
    "# Can it suggest if the document is worth to be read? Probably Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use case number 2: POS (Part of speech) tagging with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a portion of the previous report\n",
    "partial_text = clean_text[5244:6126]\n",
    "print(partial_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the Natural Language Processor for the English language\n",
    "import spacy # http://spacy.io\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') # https://spacy.io/usage/models#languages for a list of all available models\n",
    "\n",
    "# Parse the text\n",
    "doc = nlp(partial_text)\n",
    "\n",
    "# Visualise some part of the elaboration\n",
    "displacy.render(doc[0:36], style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate only Nouns in their singular form\n",
    "pos_interesting_types = ['PROPN', 'NOUN']\n",
    "ext_nouns = []\n",
    "for token in doc:\n",
    "    if (token.pos_ in pos_interesting_types):\n",
    "        ext_nouns.append(token.lemma_)\n",
    "print(doc)\n",
    "print()\n",
    "print(ext_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract verbs, excluding modal ones\n",
    "pos_interesting_types = ['VERB']\n",
    "ext_verbs = []\n",
    "for token in doc:\n",
    "    if (token.pos_ in pos_interesting_types) and (token.tag_ != 'MD'): # Exclude modal verbs\n",
    "        ext_verbs.append(token.lemma_)\n",
    "print(doc)\n",
    "print()\n",
    "print(ext_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Noun chunks (=group of words that make sense together)\n",
    "ext_chunks = []\n",
    "for nc in doc.noun_chunks:\n",
    "    ext_chunks.append(nc.text)\n",
    "print(ext_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: POS tagging can be useful as part of a processing pipeline, alone does not produce interesting results\n",
    "# Useful to produce better word clouds or textual statistical distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use case number 3: text similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert report text into a table\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "text_table = pd.DataFrame()\n",
    "for line in clean_text.split('\\n'):\n",
    "    text_table = text_table.append({'Line': line}, ignore_index=True)\n",
    "\n",
    "text_table.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordVector for each line of text\n",
    "text_table['Vector'] = text_table['Line'].apply(lambda x: nlp(x).vector)\n",
    "text_table.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: for any given sentence, search for the \"most similar\" line in the original text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#sentence = \"Cost monitoring\"\n",
    "#sentence = \"Gender balance\"\n",
    "sentence = \"Importance of primary schools\"\n",
    "# Calculate the sentence vector\n",
    "sentence_vec = nlp(sentence).vector\n",
    "# Calculate the cosine similarity (=the closeness in the 100-dimensions space) between the sentence and all other lines in the text\n",
    "similarities = cosine_similarity([sentence_vec], text_table['Vector'].to_list())\n",
    "# Which line has max similarity?\n",
    "print(text_table.loc[similarities.argmax(), 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Stanford CoreNLP\n",
    "## Live demo at https://corenlp.run/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open question: can we really satisfy the expectation of mining \"textual meaning\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open question: how to use statistical results with inevitable biases to derive audit findings?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECALab",
   "language": "python",
   "name": "ecalab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
